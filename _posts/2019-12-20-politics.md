---
title: "Machine Learning Project: Political Social Media Messages"
date: 2019-12-20
tags: [machine learning, data science, exploratory data analysis, politics]
header:
  image: "/images/deltalab.jpg"
excerpt: "Machine Learning, Politics, Data Science"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
library(dplyr)
library(eeptools)
library(gsubfn)
library(tm)
library(e1071)
library(SnowballC)
library(caret)
library(tidyr)
library(ggplot2)
library(DT)
library(knitr)
library(wordcloud)
library(tidytext)
```

## Problem Statement

As social media becomes increasingly relevant in the world today and as politicians utilize digital platforms as a political tool, we've decided to undertake a project analyzing these political posts. We had several potential questions that guided this project. Could we predict the gender, age, and/or party of a given politician based on their social media posts? Which words or terms are strongly associated with a particular major party, or both? Can unique messages from independent parties be labelled as left-leaning or right-leaning, or both? We considered these problems and questions to be even more relevant considering the upcoming 2020 presidential elections, and with candidates fighting to win the majority of their party and ultimately the country, the results of this project can offer insights into individual candidates and allow objective analysis into the political leanings and tendencies of a candidate without the subjective judgement of a human.

## Data Description

In order to tackle the questions outlined above, we used a combination of two datasets: Political Social Media Posts and U.S. Legislator Attributes.

https://www.kaggle.com/crowdflower/political-social-media-posts?fbclid=IwAR1zd5-O3bVc7oPH8ABHkiF524TM370lSZ44oR5_nQPfcwPlBFCHXbT-Vuc

https://github.com/unitedstates/congress-legislators/blob/master/README.md

The Political Social Media Posts dataset was retrieved from Kaggle. The data was collected through numerous anonymous contributors, who looked at politician’s social media posts from December 31st, 2012 to December 30th, 2014.  The anonymous contributors classified the dataset by breaking the messages down into audience (national or constituency), bias (neutral/bipartisan or biased/partisan), as well as the message itself.  The main variables of interest were coded in as string type variables, which we broke down using regular expressions in order to analyze the dataset more efficiently.

The U.S. Legislator Attributes dataset was retrieved from GitHub. The database is collected from a variety of sources but maintained primarily by volunteers from well-known firms such as ProPublica and FiveThirtyEight. Additional data is automated and imported from multiple other sources as well, such as The Congressional Biographical Directory and C-SPAN’s Congressional Chronicle.  The dataset we used was the Legislators Historical dataset, which includes all members of that ever served in Congress from 1789.  The variables of interest that we used for our analysis were the age, party affiliation, gender and state affiliation of the politician.  All the variables besides age was coded in as string types, whereas the age was formatted as an integer type.

## Data Preprocessing

**Kaggle Data:**

In terms of data preprocessing, we first dealt with missing observations by taking the column sums across our dataset. Only 4 of the 21 variables in our dataset contained missing values. Considering they were unimportant in terms of the analysis we were conducting, these columns were dropped accordingly.

```{r datainput_kaggle}
## Import Dataset:
df <- read.csv("Data/political_social_media.csv", stringsAsFactors=FALSE, fileEncoding="latin1")

## Clean Variable Names:
df %>%
  rename(unit_id = X_unit_id,
         golden = X_golden,
         unit_state = X_unit_state,
         trusted_judgements = X_trusted_judgments,
         last_judgement_at = X_last_judgment_at,
         orig_golden = orig__golden) -> df

## Deal with missing columns/data:
k = which(colSums(is.na(df))>0)
df <- df[, -k]

```

**Github Data:**

First, we imported both the historical and current datasets on U.S. legislator attributes, subsetting the variables that would be important for our analysis (e.g. Full Name, Gender, Party, Bio ID, and Birthday). We then used rbind() to bind the rows of these two datasets so we could get a complete set of all U.S. legislators with their attributes. Finally we used a left_join to merge the data-managed legislator attributes to the Kaggle dataset.

```{r datainput_github}
## Add Politician Attributes:
## source: Github
vars = c("full_name", "gender", "party", "bioguide_id", "birthday")
leg_historical <- read.csv("Data/legislators-historical.csv", na.strings = "")[,vars]
leg_current <- read.csv("Data/legislators-current.csv", na.strings = "")[,vars]
legislators <- rbind(leg_historical, leg_current)
```

**Completed Data Set:**

Once we had our full dataset, we were able to begin the necessary data preprocessing steps for analyzing the text content of politicians’ social media messages (“text” column). First we created a corpus from the text column of our dataset, which contains the text content of each politician’s social media message. To clean the text corpus we performed the following management steps:

* Set text to lowercase

* Remove numbers

* Remove stopwords (e.g. “a”, “and”, “but”, etc.)

* Remove punctuation

* Stripped unnecessary white spaces

* Stemmed the words (“learning” or “learned” => “learn”)

After cleaning the text content of our data, we then converted our text corpus to a document term matrix, which creates columns for each individual term in our corpus and a corresponding frequency of references for each row of our initial data set. From here we performed a **75/25 training/testing split** on our document term matrix and on the relevant dependent variables for our ML model (e.g. gender, political party). We then decided to only include terms that occurred at least 5 unique messages within the document term matrix and converted word counts to 1’s and 0’s, rather than frequency of word use per row. This was a necessary step because Naive Bayes requires categorical predictors, e.g. word occurrence rather than word frequency, therefore we had to convert the word variables to “Yes” or “No” (1,0) factors.


```{r datainput_joined}
## Join Data Sets:
df %>%
  left_join(legislators, by = c("bioid" = "bioguide_id")) -> temp_full

## Calculate legislator ages:
temp_full$birthday <- as.Date(temp_full$birthday)
temp_full$age <- floor(age_calc(temp_full$birthday, Sys.Date(), units = "years"))

## Select Variables:
temp_full %>%
  select(audience, bias, message, source, text, full_name, gender, party, age) -> temp_full

# 1. convert party variable to factor
temp_full$party <- factor(temp_full$party)

# 2. build a corpus using tm package
df_corpus <- Corpus(VectorSource(temp_full$text))

# 3. clean up corpus using tm_map

corpus_clean <- tm_map(df_corpus, tolower) # set text to lower
corpus_clean <- tm_map(corpus_clean, removeNumbers) # remove numbers from text
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords()) # remove stopwords
corpus_clean <- tm_map(corpus_clean, removePunctuation) # remove punct
corpus_clean <- tm_map(corpus_clean, stripWhitespace) # remove unnecessary white spaces

# 4. stem word variants (e.g. learning, learned, learnin)
corpus_clean <- tm_map(corpus_clean, stemDocument)

# 5. create document term matrix
politic_dtm <- DocumentTermMatrix(corpus_clean)

# create training and testing datasets
politic_dtm_train <- politic_dtm[1:3750, ] # 75% of data in training
politic_dtm_test <- politic_dtm[3751:5000, ] # 25% of data in training

# collect labels (party affiliation)
politic_train_labels <- temp_full$party[1:3750]
politic_test_labels <- temp_full$party[3751:5000]


# indicator feature for frequent words
politic_freq_words <- findFreqTerms(politic_dtm_train, 5)
# returns words that have appeared in at least 5 messages/emails

# subset training and testing datasets to only include these words...
politic_dtm_freq_train <- politic_dtm_train[, politic_freq_words]
politic_dtm_freq_test <- politic_dtm_test[, politic_freq_words]

# convert counts to a factor
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x,
              levels = c(0,1),
              labels = c("No", "Yes"))
  return(x)
}

# apply convert_counts() to columns of the train/test data
politic_train <- apply(politic_dtm_freq_train, 2, convert_counts)
# when using the apply() function on matrices, 1 refers to rows, 2 refers to columns.
politic_test <- apply(politic_dtm_freq_test, 2, convert_counts)

```



## Machine Learning Approach

We used the Naive Bayes technique to analyze the structured text dataset. The Naive Bayes model is the de facto standard for text classification, simply given the nature of the dataset’s properties. A text classification dataset has a large number of features that each have a small impact on the final outcome, but these variables combined can have a large impact. The Naive Bayes model with text classification uses the frequencies of the words as its features to classify an observation (in our case social media posts) into a category (such as political party, gender). Naive Bayes, if run on a single feature (word), works by calculating the probability that an observation is is in a specific category by multiplying the likelihood and prior probability and dividing it by the marginal livelihood. To explain these terms, consider an example trying to predict that an email is spam considering that it has the word “free” in its contents.

> $P(spam \mid “free”)$ = $\frac{(P(“free” | spam) * P(spam))} {P(“free”)}$

The likelihood is the probability that an observation that is spam has the word “free”. The prior probability is the probability that an observation is spam. The marginal livelihood is the probability than an observation has the word “free”. This calculates the goal posterior probability which is the probability that given the word “free” an observation is spam. However, obviously text classification works with observations that have more than one word, and thus works with many features. Naive Bayes deals with multiple features (words) by assuming that all of the features are of equal importance and the features are independent of each other, i.e. the presence of one word does not affect the presence of other words (hence the term Naive). So, the Naive Bayes model simply multiplies the likelihood of features in the equations and divides it by the total marginal livelihood of the features to be able to offer a probability that a word is in a specific category given multiple features. This is a basic understanding of how the Naive Bayes model works on features for text classification into categories, and it is what we used to classify political social media posts into categories such as party and gender.


## Results

First we begin with some exploratory data analysis. After our substantial process of data preprocessing, we were able to extract the top ten frequently used words across the 5,000 messages.

```{r Table_1}
words_test_set <-as.data.frame(as.matrix(politic_test))

words_test_set <- data.frame(apply(words_test_set, 2, function(x) {as.numeric(as.character(ifelse(x == "Yes", 1, 0)))}))

words_test_set <- cbind(politic_test_labels, words_test_set)

word.frequencies <- unname(sort(colSums(aggregate(words_test_set[2:1843], by = list(words_test_set$politic_test_labels), FUN = sum)[-1]), decreasing = T))[1:10]
topwords <- names(sort(colSums(aggregate(words_test_set[2:1843], by = list(words_test_set$politic_test_labels), FUN = sum)[-1]), decreasing = T)[1:10])

party_word_comparisons <- aggregate(words_test_set[2:1843], by = list(words_test_set$politic_test_labels), FUN = function(x){round(mean(x),2)})

party_word_comparisons %>%
  select(Group.1, topwords) %>%
  rename(party = Group.1) %>%
  filter(party != "Independent") %>%
  gather(term, measurement, today:help, factor_key=TRUE) -> party_word_comparisons
#ggplot(graphdata, aes(x = term, fill = party, y = measurement)) + geom_col(position = "dodge")


data.frame(Term = unique(party_word_comparisons$term),
           Term.Frequency = word.frequencies,
           Democrat = party_word_comparisons[party_word_comparisons$party == "Democrat",]$measurement,
           Republican = party_word_comparisons[party_word_comparisons$party == "Republican",]$measurement) -> party_word_comparisons

kable(party_word_comparisons,
          caption = "Table 1. Top 10 Most Frequent Words with Party Ratios")
```

We also found the usage percentage of each of those words across both Democratic and Republican posts. Some particularly interesting words were "work" (14% of Democratic and 9% of Republican posts), "new" (12% of Democratic and 8% of Republican posts), and "today" (27% of Democratic and 19% of Republican posts). Additionally we extracted the top ten frequently used words for each party.

```{r Tables 2 & 3}
#### ------- Top 10 Terms per party ------ ####
topwords.dem <- aggregate(words_test_set[2:1843], by = list(words_test_set$politic_test_labels), FUN = mean)
topwords.dem %>% filter(Group.1 == "Democrat") %>% select(-1)-> topwords.dem
topwords.dem <- sort(colSums(topwords.dem), decreasing = T)[1:10]

topwords.rep <- aggregate(words_test_set[2:1843], by = list(words_test_set$politic_test_labels), FUN = mean)
topwords.rep %>% filter(Group.1 == "Republican") %>% select(-1)-> topwords.rep
topwords.rep <- sort(colSums(topwords.rep), decreasing = T)[1:10]

topwords.rep.df <- data.frame(Term = names(topwords.rep),
                              Frequency = paste0(round(unname(topwords.rep)*100,2),"%"))
topwords.dem.df <- data.frame(Term = names(topwords.dem),
                              Frequency = paste0(round(unname(topwords.dem)*100,2),"%"))

kable(topwords.dem.df,
          caption = "Table 2. Top 10 Most Frequent Words for Democrats")

kable(topwords.rep.df,
          caption = "Table 3. Top 10 Most Frequent Words for Republicans")

## Intersecting Terms
#intersect(names(topwords.dem), names(topwords.rep))
```


Again, it was revealing to discover that words such as “new”, “community”, and “help”" were unique to the most frequent Democratic words and words such as “house”, “law”, “nation”, and “bill” were unique to the most frequent Republican words. The explanatory analysis made sense as these usage of terms align with the ideological leanings of each party towards liberal or conservative.

Afterwards we moved to apply our training data to the aforementioned Naive Bayes technique.

**Predicting Party from Social Media Messages**

```{r ML Analysis}
set.seed(1234)
politic_classifier <- naiveBayes(politic_train, politic_train_labels, laplace = 1)

# evaluate performance of model
politic_test_pred <- predict(politic_classifier, politic_test)

confusionMatrix(politic_test_labels, politic_test_pred, positive = "Democrat")
```

Our Naive Bayes model performed on the testing data with an accuracy of 0.6288 (classified 62.88% of the observations to the correct party). The sensitivities (proportion of actual positives that are correctly identified as such) were 0.5882 for Democrats, 0.7299 for Republicans, and 0.1413 for Independents. The specificities (proportion of actual negatives that are correctly identified as such) were 0.7189 for Democrats, 0.9940 for Independents, and 0.5864 for Republicans. Thus, more often than not our model correctly predicted a social media post’s party, but it was better at predicting if a Republican message was Republican and at knowing that a message was not Democratic.

We also trained and predicted on gender using the same Naive Bayes technique.

**Predicting Gender from Social Media Messages**

```{r ML Analysis Gender}
# 1. convert party variable to factor
temp_full$gender <- factor(temp_full$gender)

# collect labels (gender)
gender_train_labels <- temp_full$gender[1:3750]
gender_test_labels <- temp_full$gender[3751:5000]

# model data (Yeah!)
set.seed(1234)
gender_classifier <- naiveBayes(politic_train, gender_train_labels, laplace = 1)

# evaluate performance of model
gender_test_pred <- predict(gender_classifier, politic_test)

confusionMatrix(gender_test_labels, gender_test_pred, positive = "F")
```

The model had an accuracy of 0.6968, and a sensitivity of 0.2811 and a specificity of 0.8002 when considering the “positive” class to be female. This means that it predicted about 70% of messages correctly, while predicting 80% of male messages correctly but only 28% of female messages correctly.



## Supplemental Exploratory Analysis

Following our interpretation of the first Naive Bayes model, predicting political party, we decided to analyze the false negatives for the Independent party. By analyzing false negatives, we were able to see our model's predicted probabilities for both Democrat and Republican on a given Independent politician's social media messages. As an exploratory approach, the interpretation of false negatives was interesting as it could potentially articulate whether a given Independent's message leaned more towards the left (Democratic) or towards the right (Republican).

After filtering by "Independent", we discovered that the only politician in our testing set registered as an Independent was Bernie Sanders, therefore our initial hypothesis was that his false negative classifications would lean towards a larger predicted probability for Democrat than for Republican, as he is known to have more progressive, leftist politics.

```{r False Negative Analysis}
party_predicted_probs <- as.data.frame(predict(politic_classifier, politic_test, type = "raw"))
party_predicted_probs <- round(party_predicted_probs, 4)

independent.compare <- cbind(party_predicted_probs, politic_test_labels, temp_full$full_name[3751:5000])

d <- which(independent.compare$politic_test_labels=="Independent" & independent.compare$Independent != 1)

independent.compare %>%
  filter(politic_test_labels == "Independent", Independent != 1) %>%
  cbind(temp_full$text[3751:5000][d]) -> independent.table

col.names.independent = c("Democrat", "Independent", "Republican",
                          "Party", "Name", "Text")

kable(independent.table,
          col.names = col.names.independent,
          caption = "Table 4. Independent False Negative 'party leanings'")
```

Naive Bayes works by having many features with small impacts coming together to have a large impact, but with these messages (particularly rows 2 through 6) there are so few words and features that just a few features are having large impacts, which goes against the foundational theory of Naive Bayes. It is important to note that when working with social media data from politicians, many of the messages reflect the brevity of those in lines 2-6. Taking this into consideration, it may be useful to use list-wise deletion or imputation on the DTM matrix by setting a row sum cuttoff point. This could effectively ensure that the training/testing sets have more features per row to learn from.

**Table 4** does provide interesting insights into how our model classifies parties. The first row is an interesting example, as the text content of the message touches on policies that both materially and ideologically align with a more leftist form of politics. Our model classified this row as "Democrat" (.997 probability); "Republican" (.029 probability).

**Word Cloud: Frequencies of Democratic and Republican Term Usage**

As a final exploratory data analysis technique, we decided to create a visualization showing terms frequently used by Democrats versus Republicans.

```{r Word Cloud}
###### Comparison Cloud
cloud.words <-as.data.frame(as.matrix(politic_test))

cloud.words <- data.frame(apply(cloud.words, 2, function(x) {as.numeric(as.character(ifelse(x == "Yes", 1, 0)))}))

cloud.set <- cbind(politic_test_labels, cloud.words)

cloud.set.sums <- aggregate(cloud.set[2:1843], by = list(cloud.set$politic_test_labels), FUN = sum)

## Subset for words that occur more than 10 times:
cloud.freq.10 <- which(colSums(cloud.set.sums[-1])>10)
cloud.set.sums <- cloud.set.sums[,c("Group.1", names(cloud.freq.10))]

cloud.set.sums %>%
  gather(Term, Count, obamacar:howev) %>%
  filter(Group.1 != "Independent") -> cloud.set.key

cloud.data <- data.frame(Democrat = cloud.set.key[cloud.set.key$Group.1 == "Democrat",]$Count,
                         Republican = cloud.set.key[cloud.set.key$Group.1 == "Republican",]$Count,
                         row.names = unique(cloud.set.key$Term))

par(mfrow=c(1,1))
set.seed(1234)
comparison.cloud(as.matrix(cloud.data), random.order=FALSE, colors = c("lightsteelblue3","indianred3"),
                 title.size=2.5, max.words=400)
```

<img src="{{ site.url }}{{ site.baseurl }}/image/partywordcloud.jpg" alt="">

Words that we found particularly interesting were "communities", "immigration", "equal", "protect", and "benefit" for **Democrats** and "conservative", "pass", "committee", "stop", and "fox" for **Republicans**. We found these terms interesting as they tend to align more closely with the ideologies of their respective parties. Additionally, many of the terms that appeared on either side of the word cloud often appear in the rhetoric articulated by politicans of the respective parties. One last term that we found interesting was the frequency of Republicans using the term "Obamacare", considering this was a health care policy implemented under a Democratic presidency. For further research, a sentiment analysis could potentially show whether Republicans are using the term in a positive or negative light (our hypothesis would most likely align with Republican sentiments being negative).

## Discussion

Our model predicted relatively well on parties with an accuracy of 63%. It was better at correctly identifying Republican messages than Democratic ones, and a possible theory for this difference may be that in the political realm of when this data was taken, Barack Obama had been reelected and thus Republican politicians may have had stronger and more unique words that they used that made it easier for our model to correctly identify Republican messages as Republican. Additionally the model performed poorly on Independent messages, but this is likely due to the much smaller number of Independent messages that we had available to train as compared to the many Democratic and Republican ones.

For gender, our model performed better with an accuracy of almost 70%. It did very well in predicting male messages (80%), but poorly on predicting female messages (28%). Like the Independent party, this may be to the fact that we had many more male messages to train on than female messages, and is reflective of the gender imbalance in Congress as well.

The implications of our model is that rhetoric has a strong relation to party preferences. The model can be used to highlight and classify specific third party affiliations to determine if certain voters or individuals may be more left or right leaning. Politicians can use this information in order to target individuals for various political purposes, such as soliciting donations or to target new members for their own personal constituencies.  

In addition, the model can help politicians understand that their prose has an impact on their appearance on the political spectrum.  In order for politicians to appear more bipartisan, this model could help highlight specific words that may have a large impact on their political standing.

Furthermore, to understand how words have an impact on political standings, it is important to consider that rhetoric and syntax changes over time. Etymology, the study of words and how they change over time, plays a large role in how people communicate. With this being said, and considering the dataset was from a shorter time period (2 years), the model may not necessarily help predict political preferences far in the future, or even in the distant past. In order to generate a more accurate model for prediction in future cases, text data must be continuously added and removed in conjunction with time.


## References

Kabakoff, Robert. “Naive Bayes.” Applications of Machine Learning in Data Analysis, 2 Oct. 2019. Wesleyan University. Lecture.
